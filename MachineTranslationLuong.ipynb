{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslationLuong.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBFM_mNuKRAu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "818755dd-2de2-4966-d3df-1ce432b11f4b"
      },
      "source": [
        "!pip install rich\n",
        "!pip install contractions\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import contractions\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import rich\n",
        "from rich.progress import track\n",
        "import spacy\n",
        "\n",
        "# Constants\n",
        "\n",
        "class params():\n",
        "    pass\n",
        "\n",
        "params.batch_size = 64\n",
        "\n",
        "params.embed_size = 300\n",
        "params.gru_units = 128\n",
        "params.learning_rate = .001\n",
        "params.optimizer = tf.keras.optimizers.Adam(params.learning_rate, clipvalue=1)\n",
        "params.epochs = 100\n",
        "\n",
        "\n",
        "params.num_samples = 30000 \n",
        "params.eng_vocab = 5776\n",
        "params.ger_vocab = 8960\n",
        "params.dec_max_len = 17\n",
        "params.en_max_len = 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rich in /usr/local/lib/python3.6/dist-packages (8.0.0)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from rich) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0,>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from rich) (3.7.4.3)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from rich) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from rich) (0.9.1)\n",
            "Requirement already satisfied: dataclasses<0.8,>=0.7; python_version >= \"3.6\" and python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from rich) (0.7)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAzfxr01LXkE"
      },
      "source": [
        "#Preprocessing Text\n",
        "class preprocess_text():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def remove_pattern(self, text, pattern= r'[^a-zA-Z0-9.!?, ]', replace_with= \"\"):\n",
        "        return re.sub(pattern, replace_with, text)\n",
        "    \n",
        "    def tokenize_sent(self, text, nlp):\n",
        "        doc= nlp(text)\n",
        "        return [sent.text for sent in doc.sents]\n",
        "    \n",
        "    def tokenize_words(self, text, nlp):\n",
        "        doc= nlp(text)\n",
        "        return \" \".join(tok.text for tok in doc)\n",
        "    \n",
        "    def expand_contractions(self, text):\n",
        "\n",
        "        return contractions.fix(text)\n",
        "        \n",
        "    def do_lemmatization(self, text, nlp):\n",
        "        doc= nlp(text)\n",
        "        return ' '.join(tok.lemma_ if tok.lemma_ != \"-PRON-\" else tok.text for tok in doc)\n",
        "        \n",
        "    def add_sos_eos(self, text, sos= False, eos= False):\n",
        "        if (sos and eos):\n",
        "            return \"<sos> \" + text + \" <eos>\" \n",
        "        if eos:\n",
        "            return text + \" <eos>\"\n",
        "        if sos:\n",
        "            return \"<sos> \" + text\n",
        "        return text\n",
        "        \n",
        "    def remove_accents(self, text):\n",
        "\n",
        "        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('UTF-8', 'ignore')\n",
        "\n",
        "def call_preprocessing(df_col, nlp_en= True, lower_= True, remove_pattern_= False, tokenize_words_= False,\n",
        "               expand_contractions_= False, do_lemmatization_= False,\n",
        "               sos= False, eos= False, remove_accents_= False):\n",
        "    \n",
        "    nlp= spacy.load('en_core_web_sm') if nlp_en else spacy.load('de_core_news_sm')\n",
        "    prep= preprocess_text()\n",
        "    \n",
        "    if expand_contractions_:\n",
        "        df_col= df_col.map(lambda text: prep.expand_contractions(text))\n",
        "        \n",
        "    if remove_accents_:\n",
        "        df_col= df_col.map(lambda text: prep.remove_accents(text))\n",
        "        \n",
        "    if do_lemmatization_:\n",
        "        df_col= df_col.map(lambda text: prep.do_lemmatization(text, nlp))\n",
        "        \n",
        "    if tokenize_words_:\n",
        "        df_col= df_col.map(lambda text: prep.tokenize_words(text, nlp))\n",
        "        \n",
        "    if remove_pattern_:\n",
        "        df_col= df_col.map(lambda text: prep.remove_pattern_(text))\n",
        "    \n",
        "    if eos or sos:\n",
        "        df_col= df_col.map(lambda text: prep.add_sos_eos(text, sos, eos))\n",
        "        \n",
        "\n",
        "    if lower_:\n",
        "        df_col= df_col.map(lambda text: text.lower())\n",
        "    return df_col\n",
        "\n",
        "def tokenizer(df_col, nlp_en= True):\n",
        "    vocab= set()\n",
        "    _= [[vocab.update([tok]) for tok in text.split(\" \")] for text in df_col]\n",
        "\n",
        "    if not nlp_en:\n",
        "        vocab.update([\"<sos>\"])\n",
        "        vocab.update([\"<eos>\"])\n",
        "\n",
        "    tokenize= dict(zip(vocab, range(1, 1+len(vocab))))\n",
        "    detokenize= dict(zip(range(1, 1+len(vocab)), vocab))\n",
        "    return tokenize, detokenize, len(vocab)\n",
        "\n",
        "def padding(txt_toks, max_len):\n",
        "    curr_ls= txt_toks.split(\" \")\n",
        "    len_ls= len(curr_ls)\n",
        "    _= [curr_ls.append(\"<pad>\") for i in range(max_len-len_ls) if len(curr_ls)<max_len]\n",
        "    return \" \".join(curr_ls)\n",
        "\n",
        "def make_minibatches(df, col1= 'rev_eng_tok', col2= 'teach_force_tok', col3= 'target_tok'):\n",
        "    enc_seq= np.array([df[col1].values[i] for i in range(len(df[col1]))])\n",
        "    enc_seq= tf.data.Dataset.from_tensor_slices(enc_seq).batch(params.batch_size)\n",
        "\n",
        "    teach_force_seq= np.array([df[col2].values[i] for i in range(len(df[col2]))])\n",
        "    teach_force_seq= tf.data.Dataset.from_tensor_slices(teach_force_seq).batch(params.batch_size)\n",
        "\n",
        "    y= np.array([df[col3].values[i] for i in range(len(df[col3]))])\n",
        "    y= tf.data.Dataset.from_tensor_slices(y).batch(params.batch_size)\n",
        "    return enc_seq, teach_force_seq, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2janYXA1Eya6"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, params):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embed = tf.keras.layers.Embedding(input_dim=params.eng_vocab,\n",
        "                                               output_dim=params.embed_size)\n",
        "        \n",
        "        self.gru1 = tf.keras.layers.GRU(params.gru_units, kernel_initializer='glorot_normal',\n",
        "                                        return_sequences=True, return_state=True)\n",
        "        \n",
        "        self.gru2 = tf.keras.layers.GRU(params.gru_units, kernel_initializer='glorot_normal',\n",
        "                                        return_sequences=True, return_state=True)\n",
        "        \n",
        "    def call(self, input_seq):\n",
        "\n",
        "        x = self.embed(input_seq)\n",
        "\n",
        "        output_seq1, hidden1 = self.gru1(x)\n",
        "\n",
        "        output_seq2, hidden2 = self.gru2(output_seq1)\n",
        "\n",
        "        return output_seq2, hidden1, hidden2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Attu4q5RF5YD"
      },
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, params):\n",
        "        super(LuongAttention, self).__init__()\n",
        "\n",
        "        self.tdfc = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(params.gru_units))\n",
        "\n",
        "    def call(self, en_seq, dec_out):\n",
        "        scores = tf.keras.backend.batch_dot(self.tdfc(en_seq), dec_out, axes=(2, 2))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        mul = en_seq * attention_weights\n",
        "\n",
        "        context_vector = tf.reduce_mean(mul, axis=1)\n",
        "\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opj4VzFRG7un"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, params):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embed = tf.keras.layers.Embedding(input_dim=params.ger_vocab,\n",
        "                                               output_dim=params.embed_size)\n",
        "        \n",
        "        self.gru1 = tf.keras.layers.GRU(params.gru_units, kernel_initializer='glorot_normal',\n",
        "                                        return_sequences=True, return_state=True)\n",
        "        \n",
        "        self.gru2 = tf.keras.layers.GRU(params.gru_units, kernel_initializer='glorot_normal',\n",
        "                                        return_sequences=True, return_state=True)\n",
        "        \n",
        "        self.attention = LuongAttention(params)\n",
        "\n",
        "        self.fc = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(params.ger_vocab))\n",
        "\n",
        "    def call(self, enc_seq, teach_force_seq, init_hidden1, init_hidden2):\n",
        "\n",
        "        x = self.embed(teach_force_seq)\n",
        "\n",
        "        output_seq1, hidden1 = self.gru1(x, initial_state=init_hidden1)\n",
        "\n",
        "        output_seq2, hidden2 = self.gru2(output_seq1, initial_state=init_hidden2)\n",
        "\n",
        "        context_vector, attention_weights = self.attention(enc_seq, output_seq2)\n",
        "\n",
        "        x = tf.concat([output_seq2, tf.expand_dims(context_vector, 1)], axis= -1)\n",
        "\n",
        "        x = tf.nn.tanh(x)\n",
        "\n",
        "        y = self.fc(x)\n",
        "\n",
        "        return y, hidden1, hidden2, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUVx7wVQIsri"
      },
      "source": [
        "def loss(y, ypred, sce):\n",
        "\n",
        "    loss_ = sce(y, ypred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y, 0), tf.float32)\n",
        "\n",
        "    loss_ = mask * loss_\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1MGmTylI9Is"
      },
      "source": [
        "@tf.function\n",
        "def train_step(params, x, ger_inp, ger_out, encoder, decoder, sce):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        tot_loss = 0\n",
        "\n",
        "        enc_seq, hidden1, hidden2 = encoder(x)\n",
        "\n",
        "        for i in range(params.dec_max_len):\n",
        "\n",
        "            dec_inp = tf.expand_dims(ger_inp[:, i], axis=1)\n",
        "\n",
        "            ypred, hidden1, hidden2, attention_weights = decoder(enc_seq, dec_inp, hidden1, hidden2)\n",
        "\n",
        "            timestep_loss = loss(tf.expand_dims(ger_out[:, i], 1), ypred, sce)\n",
        "\n",
        "            tot_loss += timestep_loss\n",
        "\n",
        "        avg_timestep_loss = tot_loss/params.dec_max_len\n",
        "\n",
        "    total_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    grads = tape.gradient(avg_timestep_loss, total_vars)\n",
        "    params.optimizer.apply_gradients(zip(grads, total_vars))\n",
        "\n",
        "    return grads, avg_timestep_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcwBRjHKTZw"
      },
      "source": [
        "def save_checkpoints(params, encoder, decoder):\n",
        "    checkpoint_dir = '/content/model_checkpoints'\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    ckpt = tf.train.Checkpoint(optimizer=params.optimizer,\n",
        "                               encoder=encoder,\n",
        "                               decoder=decoder)\n",
        "    ckpt.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "def restore_checkpoint(params, encoder, decoder):\n",
        "    checkpoint_dir = '/content/model_checkpoints'\n",
        "    ckpt= tf.train.Checkpoint(optimizer=params.optimizer,\n",
        "                              encoder=encoder,\n",
        "                              decoder=decoder)\n",
        "    ckpt.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66eE7vhErgCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30aace8f-27bf-4da1-a7f2-44e9068eaed8"
      },
      "source": [
        "df = pd.read_csv('/content/eng2ger.csv')\n",
        "\n",
        "tokenize_eng, detokenize_eng, len_eng= tokenizer(df['eng_input'], True)\n",
        "tokenize_ger, detokenize_ger, len_ger= tokenizer(df['ger_input'], False)\n",
        "\n",
        "tokenize_eng['<pad>'] = 0\n",
        "detokenize_eng[0] = \"<pad>\"\n",
        "tokenize_ger[\"<pad>\"] = 0\n",
        "detokenize_ger[0] = \"<pad>\"\n",
        "\n",
        "\n",
        "num_samples = df.shape[0]\n",
        "eng_vocab = len_eng + 1\n",
        "ger_vocab = len_ger + 1 \n",
        "\n",
        "\n",
        "df['eng_input'] = df['eng_input'].map(lambda txt: padding(txt, params.en_max_len))\n",
        "df['ger_input'] = df['ger_input'].map(lambda txt: padding(txt, params.dec_max_len))\n",
        "df['ger_target'] = df['ger_target'].map(lambda txt: padding(txt, params.dec_max_len))\n",
        "\n",
        "df['eng_tok'] = df['eng_input'].map(lambda txt: [tokenize_eng[tok] for tok in txt.split(' ')])\n",
        "\n",
        "df['teach_force_tok'] = df['ger_input'].map(lambda txt: [tokenize_ger[tok] for tok in txt.split(' ')])\n",
        "df['target_tok'] = df['ger_target'].map(lambda txt: [tokenize_ger[tok] for tok in txt.split(' ')])\n",
        "\n",
        "df['rev_eng_tok'] = df['eng_tok'].map(lambda ls: ls[:: -1])\n",
        "\n",
        "enc_seq, teach_force_seq, y = make_minibatches(df, col1='rev_eng_tok', col2='teach_force_tok', col3='target_tok')\n",
        "print(enc_seq, teach_force_seq, y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (None, 20), types: tf.int64> <BatchDataset shapes: (None, 17), types: tf.int64> <BatchDataset shapes: (None, 17), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZgbb4Ost6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11a1e38f-e16f-4cef-8547-32132b4705cb"
      },
      "source": [
        "#tf.keras.backend.clear_session()\n",
        "encoder = Encoder(params)\n",
        "decoder = Decoder(params)\n",
        "\n",
        "def train():\n",
        "\n",
        "    sce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    \n",
        "    start = time.time()\n",
        "    avg_loss = []\n",
        "\n",
        "    for e in track(range(0, params.epochs)):\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        st = time.time()\n",
        "\n",
        "        for enc_seq_batch, teach_force_seq_batch, y_batch in zip(enc_seq, teach_force_seq, y):\n",
        "\n",
        "            grads, loss = train_step(params, enc_seq_batch, teach_force_seq_batch, y_batch, encoder, decoder, sce)\n",
        "\n",
        "            losses.append(loss.numpy())\n",
        "\n",
        "        avg_loss.append(np.mean(losses))\n",
        "\n",
        "        print(f'EPOCH - {e+1} ---- LOSS - {np.mean(losses)} ---- TIME - {time.time()- st}')\n",
        "\n",
        "    save_checkpoints(params, encoder, decoder)\n",
        "    print(f'total time taken: {time.time()-start}')\n",
        "\n",
        "    return grads, avg_loss\n",
        "\n",
        "grads, avg_loss = train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH - 1 ---- LOSS - 3.0386593341827393 ---- TIME - 62.81144380569458\n",
            "EPOCH - 2 ---- LOSS - 2.6535072326660156 ---- TIME - 23.105008125305176\n",
            "EPOCH - 3 ---- LOSS - 2.5410878658294678 ---- TIME - 23.378941297531128\n",
            "EPOCH - 4 ---- LOSS - 2.4738378524780273 ---- TIME - 22.946999549865723\n",
            "EPOCH - 5 ---- LOSS - 2.371509552001953 ---- TIME - 22.54237723350525\n",
            "EPOCH - 6 ---- LOSS - 2.2057316303253174 ---- TIME - 22.759089946746826\n",
            "EPOCH - 7 ---- LOSS - 2.0625908374786377 ---- TIME - 23.128815412521362\n",
            "EPOCH - 8 ---- LOSS - 1.9562010765075684 ---- TIME - 22.614665269851685\n",
            "EPOCH - 9 ---- LOSS - 1.8663301467895508 ---- TIME - 22.657027006149292\n",
            "EPOCH - 10 ---- LOSS - 1.7836918830871582 ---- TIME - 22.776947259902954\n",
            "EPOCH - 11 ---- LOSS - 1.7059952020645142 ---- TIME - 22.822686672210693\n",
            "EPOCH - 12 ---- LOSS - 1.6364350318908691 ---- TIME - 23.543668270111084\n",
            "EPOCH - 13 ---- LOSS - 1.5675668716430664 ---- TIME - 23.049397945404053\n",
            "EPOCH - 14 ---- LOSS - 1.496752142906189 ---- TIME - 23.33316469192505\n",
            "EPOCH - 15 ---- LOSS - 1.4319692850112915 ---- TIME - 22.8739173412323\n",
            "EPOCH - 16 ---- LOSS - 1.3700952529907227 ---- TIME - 22.731373071670532\n",
            "EPOCH - 17 ---- LOSS - 1.3162074089050293 ---- TIME - 22.768637657165527\n",
            "EPOCH - 18 ---- LOSS - 1.2630494832992554 ---- TIME - 23.064775228500366\n",
            "EPOCH - 19 ---- LOSS - 1.208738088607788 ---- TIME - 22.790809392929077\n",
            "EPOCH - 20 ---- LOSS - 1.1525137424468994 ---- TIME - 23.201977014541626\n",
            "EPOCH - 21 ---- LOSS - 1.0988396406173706 ---- TIME - 23.415080308914185\n",
            "EPOCH - 22 ---- LOSS - 1.0515068769454956 ---- TIME - 23.370303869247437\n",
            "EPOCH - 23 ---- LOSS - 1.006567120552063 ---- TIME - 22.737788915634155\n",
            "EPOCH - 24 ---- LOSS - 0.9641064405441284 ---- TIME - 23.44958519935608\n",
            "EPOCH - 25 ---- LOSS - 0.9242658019065857 ---- TIME - 23.84342122077942\n",
            "EPOCH - 26 ---- LOSS - 0.8801120519638062 ---- TIME - 22.768009185791016\n",
            "EPOCH - 27 ---- LOSS - 0.8345345258712769 ---- TIME - 23.088550090789795\n",
            "EPOCH - 28 ---- LOSS - 0.7894389629364014 ---- TIME - 22.887449502944946\n",
            "EPOCH - 29 ---- LOSS - 0.7503373026847839 ---- TIME - 23.519758224487305\n",
            "EPOCH - 30 ---- LOSS - 0.7157600522041321 ---- TIME - 23.04275131225586\n",
            "EPOCH - 31 ---- LOSS - 0.6867272853851318 ---- TIME - 23.021143913269043\n",
            "EPOCH - 32 ---- LOSS - 0.6598122119903564 ---- TIME - 22.73414635658264\n",
            "EPOCH - 33 ---- LOSS - 0.6316508054733276 ---- TIME - 23.044589042663574\n",
            "EPOCH - 34 ---- LOSS - 0.603111743927002 ---- TIME - 23.18103051185608\n",
            "EPOCH - 35 ---- LOSS - 0.5762280225753784 ---- TIME - 22.998974561691284\n",
            "EPOCH - 36 ---- LOSS - 0.5521844625473022 ---- TIME - 23.014350652694702\n",
            "EPOCH - 37 ---- LOSS - 0.5307698249816895 ---- TIME - 23.2654972076416\n",
            "EPOCH - 38 ---- LOSS - 0.5090199112892151 ---- TIME - 22.81320285797119\n",
            "EPOCH - 39 ---- LOSS - 0.48973244428634644 ---- TIME - 23.37316656112671\n",
            "EPOCH - 40 ---- LOSS - 0.47165215015411377 ---- TIME - 23.233646392822266\n",
            "EPOCH - 41 ---- LOSS - 0.4505922496318817 ---- TIME - 23.33302593231201\n",
            "EPOCH - 42 ---- LOSS - 0.4257223904132843 ---- TIME - 23.34452986717224\n",
            "EPOCH - 43 ---- LOSS - 0.4012949466705322 ---- TIME - 23.448832273483276\n",
            "EPOCH - 44 ---- LOSS - 0.38107502460479736 ---- TIME - 23.10068702697754\n",
            "EPOCH - 45 ---- LOSS - 0.3599148988723755 ---- TIME - 23.072858095169067\n",
            "EPOCH - 46 ---- LOSS - 0.34205350279808044 ---- TIME - 22.645440340042114\n",
            "EPOCH - 47 ---- LOSS - 0.3253691494464874 ---- TIME - 23.511725425720215\n",
            "EPOCH - 48 ---- LOSS - 0.3111059069633484 ---- TIME - 22.99041724205017\n",
            "EPOCH - 49 ---- LOSS - 0.2998151183128357 ---- TIME - 23.2245512008667\n",
            "EPOCH - 50 ---- LOSS - 0.2877425253391266 ---- TIME - 23.12955403327942\n",
            "EPOCH - 51 ---- LOSS - 0.2773100733757019 ---- TIME - 23.938557386398315\n",
            "EPOCH - 52 ---- LOSS - 0.2651326358318329 ---- TIME - 23.792681455612183\n",
            "EPOCH - 53 ---- LOSS - 0.25564149022102356 ---- TIME - 22.98318338394165\n",
            "EPOCH - 54 ---- LOSS - 0.24455103278160095 ---- TIME - 22.72172975540161\n",
            "EPOCH - 55 ---- LOSS - 0.22876524925231934 ---- TIME - 23.26331639289856\n",
            "EPOCH - 56 ---- LOSS - 0.21382436156272888 ---- TIME - 23.758732795715332\n",
            "EPOCH - 57 ---- LOSS - 0.2002706229686737 ---- TIME - 23.11727023124695\n",
            "EPOCH - 58 ---- LOSS - 0.1886989027261734 ---- TIME - 23.809818506240845\n",
            "EPOCH - 59 ---- LOSS - 0.18127292394638062 ---- TIME - 23.234400510787964\n",
            "EPOCH - 60 ---- LOSS - 0.17469730973243713 ---- TIME - 22.764215230941772\n",
            "EPOCH - 61 ---- LOSS - 0.16798366606235504 ---- TIME - 22.8147075176239\n",
            "EPOCH - 62 ---- LOSS - 0.16019776463508606 ---- TIME - 23.6973717212677\n",
            "EPOCH - 63 ---- LOSS - 0.1530337631702423 ---- TIME - 23.24476408958435\n",
            "EPOCH - 64 ---- LOSS - 0.14766158163547516 ---- TIME - 22.5619957447052\n",
            "EPOCH - 65 ---- LOSS - 0.14275582134723663 ---- TIME - 23.10318160057068\n",
            "EPOCH - 66 ---- LOSS - 0.13702236115932465 ---- TIME - 23.356314420700073\n",
            "EPOCH - 67 ---- LOSS - 0.13232523202896118 ---- TIME - 22.7618670463562\n",
            "EPOCH - 68 ---- LOSS - 0.12776854634284973 ---- TIME - 22.75777840614319\n",
            "EPOCH - 69 ---- LOSS - 0.12323296815156937 ---- TIME - 22.7455997467041\n",
            "EPOCH - 70 ---- LOSS - 0.11624888330698013 ---- TIME - 23.518859386444092\n",
            "EPOCH - 71 ---- LOSS - 0.11051376163959503 ---- TIME - 22.498271226882935\n",
            "EPOCH - 72 ---- LOSS - 0.10325086861848831 ---- TIME - 22.395232439041138\n",
            "EPOCH - 73 ---- LOSS - 0.0972411036491394 ---- TIME - 22.839969158172607\n",
            "EPOCH - 74 ---- LOSS - 0.09441359341144562 ---- TIME - 23.168065547943115\n",
            "EPOCH - 75 ---- LOSS - 0.08645624667406082 ---- TIME - 23.219931840896606\n",
            "EPOCH - 76 ---- LOSS - 0.07888907939195633 ---- TIME - 23.41448426246643\n",
            "EPOCH - 77 ---- LOSS - 0.07410357147455215 ---- TIME - 23.038904905319214\n",
            "EPOCH - 78 ---- LOSS - 0.07092412561178207 ---- TIME - 22.40042209625244\n",
            "EPOCH - 79 ---- LOSS - 0.06871048361063004 ---- TIME - 23.5384202003479\n",
            "EPOCH - 80 ---- LOSS - 0.0681609958410263 ---- TIME - 22.76897382736206\n",
            "EPOCH - 81 ---- LOSS - 0.06601288914680481 ---- TIME - 23.476240158081055\n",
            "EPOCH - 82 ---- LOSS - 0.06357281655073166 ---- TIME - 23.225743055343628\n",
            "EPOCH - 83 ---- LOSS - 0.06157034635543823 ---- TIME - 23.3202645778656\n",
            "EPOCH - 84 ---- LOSS - 0.060782916843891144 ---- TIME - 23.66737151145935\n",
            "EPOCH - 85 ---- LOSS - 0.05830032005906105 ---- TIME - 22.58758306503296\n",
            "EPOCH - 86 ---- LOSS - 0.05385109782218933 ---- TIME - 23.38483476638794\n",
            "EPOCH - 87 ---- LOSS - 0.051662322133779526 ---- TIME - 22.422282934188843\n",
            "EPOCH - 88 ---- LOSS - 0.05025379732251167 ---- TIME - 23.027121782302856\n",
            "EPOCH - 89 ---- LOSS - 0.04831867665052414 ---- TIME - 22.951400756835938\n",
            "EPOCH - 90 ---- LOSS - 0.04514781013131142 ---- TIME - 23.482147932052612\n",
            "EPOCH - 91 ---- LOSS - 0.04360612481832504 ---- TIME - 22.960065603256226\n",
            "EPOCH - 92 ---- LOSS - 0.03959904611110687 ---- TIME - 22.999867916107178\n",
            "EPOCH - 93 ---- LOSS - 0.03677624464035034 ---- TIME - 22.766454935073853\n",
            "EPOCH - 94 ---- LOSS - 0.04070352017879486 ---- TIME - 22.86612844467163\n",
            "EPOCH - 95 ---- LOSS - 0.044739414006471634 ---- TIME - 23.832935333251953\n",
            "EPOCH - 96 ---- LOSS - 0.05450168624520302 ---- TIME - 23.090214490890503\n",
            "EPOCH - 97 ---- LOSS - 0.0470055416226387 ---- TIME - 23.7302029132843\n",
            "EPOCH - 98 ---- LOSS - 0.03984875604510307 ---- TIME - 23.084601879119873\n",
            "EPOCH - 99 ---- LOSS - 0.03495701774954796 ---- TIME - 23.496151447296143\n",
            "EPOCH - 100 ---- LOSS - 0.03234730288386345 ---- TIME - 23.702816009521484\n",
            "total time taken: 2350.976849079132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hcH0Sm8_G3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0053ce94-ce21-44b5-dc6a-a0dfe10ff844"
      },
      "source": [
        "df = pd.read_csv('/content/eng2ger.csv')\n",
        "\n",
        "tokenize_eng, detokenize_eng, params.len_eng = tokenizer(df['eng_input'], True)\n",
        "tokenize_ger, detokenize_ger, params.len_ger = tokenizer(df['ger_input'], False)\n",
        "\n",
        "def make_prediction(txt, params, greedy=False, random_sampling=True, beam_search=False):\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    txt = contractions.fix(txt)\n",
        "\n",
        "    x = tf.expand_dims(tf.constant([tokenize_eng[tok.text.lower()] for tok in nlp(txt)]), 0)\n",
        "\n",
        "    #encoder = Encoder(params)\n",
        "    #decoder = Decoder(params)\n",
        "\n",
        "    #restore_checkpoint(params, encoder, decoder)\n",
        "\n",
        "    dec_inp = tf.reshape(tokenize_ger['<sos>'], (1,1))\n",
        "    final_tok, i = '<sos>', 0\n",
        "\n",
        "    sent, att = [], []\n",
        "    enc_seq, hidden1, hidden2 = encoder(x)\n",
        "\n",
        "    while final_tok != '<eos>':\n",
        "\n",
        "        ypred, hidden1, hidden2, attention_weights = decoder(enc_seq, dec_inp, hidden1, hidden2)\n",
        "\n",
        "        if random_sampling:\n",
        "            idx = tf.random.categorical(ypred[:, 0, :], num_samples= 1)\n",
        "\n",
        "        elif greedy:\n",
        "            idx = tf.argmax(ypred[:, 0, :], axis= -1)\n",
        "\n",
        "        elif beam_search:\n",
        "            pass\n",
        "\n",
        "        sent.append(detokenize_ger[tf.squeeze(idx).numpy()])\n",
        "\n",
        "        att.append(attention_weights)\n",
        "        dec_inp = idx\n",
        "        if i == 10:\n",
        "            break\n",
        "        else:\n",
        "            i += 1\n",
        "    return \" \".join(sent), att\n",
        "\n",
        "txt = input('Type anything: ')\n",
        "sent, att = make_prediction(txt, params)\n",
        "print('[bold blue]' + sent)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type anything: hello\n",
            "[bold blue]er macht ? sage tom aufgeregt er studiert manner beschutzen benutzen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OepSdxZcbdXU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}